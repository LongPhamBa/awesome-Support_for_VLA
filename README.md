[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
# awesome - Support for VLA

## â€‹ Paper and Project

- [2025] Narrate2Nav: Real-Time Visual Navigation with Implicit Language Reasoning in Human-Centric Environments [paper](https://arxiv.org/pdf/2506.14233)

- [2025] QUAR-VLA: Vision-Language-Action Model for Quadruped Robots [paper](https://arxiv.org/pdf/2312.14457)

- [2025] MuJoCo Playground [paper](https://arxiv.org/pdf/2502.08844) [project](https://playground.mujoco.org/)

- [2025] DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control [paper](https://arxiv.org/pdf/2502.05855) [project](https://dex-vla.github.io/)

- [2025] [Nvidia] GR00T N1: An Open Foundation Model for Generalist Humanoid Robots [paper](https://arxiv.org/pdf/2503.14734) [project](https://github.com/NVIDIA/Isaac-GR00T)
  
- [2025] Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation [paper](https://arxiv.org/abs/2502.16707) [project](https://reflect-vlm.github.io/)

- [2025] MuBlE: MuJoCo and Blender simulation Environment and Benchmark for Task Planning in Robot Manipulation [paper](https://arxiv.org/abs/2503.02834)

- [2025] Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success [paper](https://arxiv.org/abs/2502.19645) [project](https://openvla-oft.github.io)

- [2025] [Figure] Helix: A Vision-Language-Action Model for Generalist Humanoid Control [report](https://www.figure.ai/news/helix)

- [2025] Genie 3: A new frontier for world models [project](https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/)

- [2025] DeepWiki openvla [project](https://deepwiki.com/openvla/openvla)

- [2024] Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs [paper](https://arxiv.org/abs/2407.07775)

- [2024] OpenVLA: An Open-Source Vision-Language-Action Model [paper](https://arxiv.org/abs/2406.09246) [project](https://github.com/reazon-research/openvla)

- [2024] TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation [paper](https://arxiv.org/abs/2409.12514) [project](https://tiny-vla.github.io)

- [2024] NaVILA: Legged Robot Vision-Language-Action Model for Navigation [paper](https://arxiv.org/abs/2412.04453)

- [2024] DextrAH-RGB: Visuomotor Policies to Grasp Anything with Dexterous Hands [paper](https://dextrah-rgb.github.io) [project](https://dextrah-rgb.github.io)

- [2024] Robosuite: A Modular Simulation Framework and Benchmark for Robot Learning [project](https://robosuite.ai)

- [2023] RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control [paper](https://arxiv.org/abs/2307.15818) [project](https://robotics-transformer2.github.io/)

- [2021] CLIP: Connecting text and images [paper](https://arxiv.org/pdf/2103.00020) [project](https://openai.com/index/clip/)


