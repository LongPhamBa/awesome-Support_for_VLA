[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
# awesome-VLA

## â€‹ Paper

- [2024] OpenVLA: An Open-Source Vision-Language-Action Model [ğŸ“„ paper](https://arxiv.org/abs/2406.09246) [ğŸ’» project](https://github.com/reazon-research/openvla)

- [2025] Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success [ğŸ“„ paper](https://arxiv.org/abs/2502.19645) [ğŸ’» project](https://openvla-oft.github.io)

- [2024] TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation [ğŸ“„ paper](https://arxiv.org/abs/2409.12514) [ğŸ’» project](https://tiny-vla.github.io)

- [2025] MuBlE: MuJoCo and Blender simulation Environment and Benchmark for Task Planning in Robot Manipulation [ğŸ“„ paper](https://arxiv.org/abs/2503.02834)

- [2024] NaVILA: Legged Robot Vision-Language-Action Model for Navigation [ğŸ“„ paper](https://arxiv.org/abs/2412.04453)

- [2024] DextrAH-RGB: Visuomotor Policies to Grasp Anything with Dexterous Hands [ğŸ“„ paper](https://dextrah-rgb.github.io) [ğŸ’» project](https://dextrah-rgb.github.io)

- [â€“] DeepWiki â€” sinh ra tá»« deepwiki project (chÆ°a rÃµ paper; lÃ  ná»n táº£ng wiki má»Ÿ) [ğŸ’» project](https://deepwiki.com/openvla/openvla)

- [â€“] Robosuite: A Modular Simulation Framework and Benchmark for Robot Learning (khÃ´ng pháº£i paper VLA) [ğŸ’» project](https://robosuite.ai)
